{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import sklearn\n",
    "\n",
    "f=open('data.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'chatbot', '(', 'also', 'known', 'as', 'a', 'spy', ',', 'conversational']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "sent_tokens[0]\n",
    "word_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'chatbot',\n",
       " 'also',\n",
       " 'known',\n",
       " 'a',\n",
       " 'a',\n",
       " 'spy',\n",
       " 'conversational',\n",
       " 'bot',\n",
       " 'chatterbot',\n",
       " 'interactive',\n",
       " 'agent',\n",
       " 'conversational',\n",
       " 'interface',\n",
       " 'conversational',\n",
       " 'ai',\n",
       " 'talkbot',\n",
       " 'or',\n",
       " 'artificial',\n",
       " 'spy',\n",
       " 'entity',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'program',\n",
       " 'or',\n",
       " 'an',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'which',\n",
       " 'conduct',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'via',\n",
       " 'auditory',\n",
       " 'or',\n",
       " 'textual',\n",
       " 'method']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#test the preprocessing function\n",
    "LemNormalize(sent_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence): \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "        \n",
    "        \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuejunli/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126, 946)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the sentences in data\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#match input to the preprocessed sentences\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    new = TfidfVec.transform([user_response])\n",
    "    vals = cosine_similarity(new[0], tfidf)\n",
    "    idx=vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the bot\n",
    "flag=True\n",
    "print(\"CHATTY: My name is CHATTY. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"CHATTY: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"CHATTY: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"CHATTY: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"CHATTY: Bye! take care..\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# install spacy and models first\n",
    "    pip install -U spacy\n",
    "    python -m spacy download en_core_web_sm    # small\n",
    "    python -m spacy download en_core_web_md    # medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#load the required model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#process a sentence\n",
    "doc1 = nlp(u\"What is the weather today in Seattle?\")  #use unicode!!\n",
    "\n",
    "#visualize the results in a browser: http://localhost:5000\n",
    "# displacy.serve(doc1, style=\"dep\")\n",
    "# displacy.serve(doc1, style=\"ent\")\n",
    "\n",
    "# http://nlp.stanford.edu:8080/corenlp/process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What what PRON WP attr is Xxxx True True\n",
      "is be AUX VBZ ROOT is xx True True\n",
      "the the DET DT det weather xxx True True\n",
      "weather weather NOUN NN nsubj is xxxx True False\n",
      "today today NOUN NN npadvmod weather xxxx True False\n",
      "in in ADP IN prep weather xx True True\n",
      "Seattle Seattle PROPN NNP pobj in Xxxxx True False\n",
      "? ? PUNCT . punct is ? False False\n",
      "today 20 25 DATE\n",
      "Seattle 29 36 GPE\n"
     ]
    }
   ],
   "source": [
    "#detailed results\n",
    "for token in doc1:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43775487742397196\n",
      "0.7431442177902458\n",
      "king True 7.1417456 False king PROPN NNP compound xxxx True False\n",
      "queen True 6.8297405 False queen PROPN NNP compound xxxx True False\n",
      "man True 6.352939 False man PROPN NNP compound xxx True False\n",
      "woman True 6.8987513 False woman NOUN NN ROOT xxxx True False\n",
      "king king 1.0\n",
      "king queen 0.72526103\n",
      "king man 0.40884617\n",
      "king woman 0.26556593\n",
      "queen king 0.72526103\n",
      "queen queen 1.0\n",
      "queen man 0.27109137\n",
      "queen woman 0.40660653\n",
      "man king 0.40884617\n",
      "man queen 0.27109137\n",
      "man man 1.0\n",
      "man woman 0.7401744\n",
      "woman king 0.26556593\n",
      "woman queen 0.40660653\n",
      "woman man 0.7401744\n",
      "woman woman 1.0\n",
      "0.6995620076752264\n",
      "0.9084785787989824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuejunli/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/yuejunli/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "nlpd = spacy.load('en_core_web_md')\n",
    "doc2 = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc3 = nlp(u\"What's the time now in Singapore?\")\n",
    "print(doc2.similarity(doc1))\n",
    "print(doc3.similarity(doc1))\n",
    "\n",
    "#load the model with word vectorsï¼Œ which enables more accurate semantic similarity comparison   \n",
    "tokens = nlpd(u'king queen man woman')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "        \n",
    "doc_d = nlpd(u\"What is the weather today in Seattle?\")\n",
    "doc2_d = nlpd(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc3_d = nlpd(u\"What's the time now in Singapore?\")\n",
    "print(doc2_d.similarity(doc_d))\n",
    "print(doc3_d.similarity(doc_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
